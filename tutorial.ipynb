{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "52240e604bec8672",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-19T16:52:53.680525Z",
     "start_time": "2024-12-19T16:52:53.432881Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Path to dataset files: /Users/andrewgazelka/.cache/kagglehub/datasets/chitwanmanchanda/fraudulent-transactions-data/versions/1\n",
      "['Fraud.csv']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "import kagglehub\n",
    "\n",
    "# Download latest version\n",
    "directory = kagglehub.dataset_download(\"chitwanmanchanda/fraudulent-transactions-data\")\n",
    "path = os.path.join(directory, \"Fraud.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bea01f9df90853a1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-19T16:50:43.935473Z",
     "start_time": "2024-12-19T16:50:43.504374Z"
    }
   },
   "outputs": [],
   "source": [
    "from daft.daft import connect_start\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# start Daft Connect (spark-connect compatible server)\n",
    "handle = connect_start(\"sc://0.0.0.0:15003\")\n",
    "\n",
    "# create a Spark Connect client connected to the Daft Connect server\n",
    "spark = SparkSession.builder.remote(\"sc://localhost:15003\").appName(\"DaftPySparkExample\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "da1cb80997b3f915",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-19T16:50:46.268563Z",
     "start_time": "2024-12-19T16:50:46.216987Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/andrewgazelka/Projects/daft-connect-example/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🗡️ 🐟 Limit: 00:00 0 rows received, 0 rows emitted\n",
      "🗡️ 🐟 InMemory: 00:00 0 rows emitted\u001b[A\n",
      "╭───────┬──────┬────────┬──────┬───────╮          \n",
      "│ _1    ┆ _2   ┆ _3     ┆ _4   ┆ _5    │\n",
      "│ ---   ┆ ---  ┆ ---    ┆ ---  ┆ ---   │\n",
      "│ Int64 ┆ Utf8 ┆ Utf8   ┆ Utf8 ┆ Int64 │\n",
      "╞═══════╪══════╪════════╪══════╪═══════╡\n",
      "│ 2021  ┆ test ┆ Albany ┆ M    ┆ 42    │\n",
      "╰───────┴──────┴────────┴──────┴───────╯\n"
     ]
    }
   ],
   "source": [
    "data = [[2021, \"test\", \"Albany\", \"M\", 42]]\n",
    "columns = [\"Year\", \"First_Name\", \"County\", \"Sex\", \"Count\"]\n",
    "\n",
    "df1 = spark.createDataFrame(data, schema=\"Year int, First_Name STRING, County STRING, Sex STRING, Count int\")\n",
    "df1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d7f471d89dedf29b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-19T16:54:00.003220Z",
     "start_time": "2024-12-19T16:53:59.848698Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ignoring is_streaming: false\n",
      "Ignoring schema: \"\"; not yet implemented\n",
      "Ignoring options: {\"inferSchema\": \"True\", \"header\": \"True\", \"sep\": \",\"}; not yet implemented\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🗡️ 🐟 Limit: 00:00 0 rows received, 0 rows emitted\n",
      "🗡️ 🐟 ScanTask: 00:00 0 rows emitted\u001b[A\n",
      "╭───────┬──────────┬───────────┬────────────┬────────────────┬─────────┬────────────────╮\n",
      "│ step  ┆ type     ┆ amount    ┆      …     ┆ newbalanceDest ┆ isFraud ┆ isFlaggedFraud │\n",
      "│ ---   ┆ ---      ┆ ---       ┆            ┆ ---            ┆ ---     ┆ ---            │\n",
      "│ Int64 ┆ Utf8     ┆ Float64   ┆ (5 hidden) ┆ Float64        ┆ Int64   ┆ Int64          │\n",
      "╞═══════╪══════════╪═══════════╪════════════╪════════════════╪═════════╪════════════════╡\n",
      "│ 1     ┆ PAYMENT  ┆ 9839.64   ┆ …          ┆ 0              ┆ 0       ┆ 0              │\n",
      "├╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌┤\n",
      "│ 1     ┆ PAYMENT  ┆ 1864.28   ┆ …          ┆ 0              ┆ 0       ┆ 0              │\n",
      "├╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌┤\n",
      "│ 1     ┆ TRANSFER ┆ 181       ┆ …          ┆ 0              ┆ 1       ┆ 0              │\n",
      "├╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌┤\n",
      "│ 1     ┆ CASH_OUT ┆ 181       ┆ …          ┆ 0              ┆ 1       ┆ 0              │\n",
      "├╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌┤\n",
      "│ 1     ┆ PAYMENT  ┆ 11668.14  ┆ …          ┆ 0              ┆ 0       ┆ 0              │\n",
      "├╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌┤\n",
      "│ …     ┆ …        ┆ …         ┆ …          ┆ …              ┆ …       ┆ …              │\n",
      "├╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌┤\n",
      "│ 1     ┆ CASH_OUT ┆ 229133.94 ┆ …          ┆ 51513.44       ┆ 0       ┆ 0              │\n",
      "├╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌┤\n",
      "│ 1     ┆ PAYMENT  ┆ 1563.82   ┆ …          ┆ 0              ┆ 0       ┆ 0              │\n",
      "├╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌┤\n",
      "│ 1     ┆ PAYMENT  ┆ 1157.86   ┆ …          ┆ 0              ┆ 0       ┆ 0              │\n",
      "├╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌┤\n",
      "│ 1     ┆ PAYMENT  ┆ 671.64    ┆ …          ┆ 0              ┆ 0       ┆ 0              │\n",
      "├╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌┤\n",
      "│ 1     ┆ TRANSFER ┆ 215310.3  ┆ …          ┆ 0              ┆ 0       ┆ 0              │\n",
      "╰───────┴──────────┴───────────┴────────────┴────────────────┴─────────┴────────────────╯\n"
     ]
    }
   ],
   "source": [
    "df = spark.read.csv(path, header=True, inferSchema=True, sep=\",\")\n",
    "\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "52d8765edba4d381",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-19T16:54:10.340750Z",
     "start_time": "2024-12-19T16:54:10.205934Z"
    }
   },
   "outputs": [
    {
     "ename": "SparkConnectGrpcException",
     "evalue": "<_MultiThreadedRendezvous of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"Error in Daft server: Failed to show string\n\nCaused by:\n    Unsupported relation type: WithColumnsRenamed(WithColumnsRenamed { input: Some(Relation { common: Some(RelationCommon { source_info: \"\", plan_id: Some(14), origin: None }), rel_type: Some(WithColumnsRenamed(WithColumnsRenamed { input: Some(Relation { common: Some(RelationCommon { source_info: \"\", plan_id: Some(13), origin: None }), rel_type: Some(WithColumnsRenamed(WithColumnsRenamed { input: Some(Relation { common: Some(RelationCommon { source_info: \"\", plan_id: Some(12), origin: None }), rel_type: Some(WithColumnsRenamed(WithColumnsRenamed { input: Some(Relation { common: Some(RelationCommon { source_info: \"\", plan_id: Some(11), origin: None }), rel_type: Some(WithColumnsRenamed(WithColumnsRenamed { input: Some(Relation { common: Some(RelationCommon { source_info: \"\", plan_id: Some(10), origin: None }), rel_type: Some(WithColumnsRenamed(WithColumnsRenamed { input: Some(Relation { common: Some(RelationCommon { source_info: \"\", plan_id: Some(9), origin: None }), rel_type: Some(WithColumnsRenamed(WithColumnsRenamed { input: Some(Relation { common: Some(RelationCommon { source_info: \"\", plan_id: Some(8), origin: None }), rel_type: Some(WithColumnsRenamed(WithColumnsRenamed { input: Some(Relation { common: Some(RelationCommon { source_info: \"\", plan_id: Some(6), origin: None }), rel_type: Some(Read(Read { is_streaming: false, read_type: Some(DataSource(DataSource { format: Some(\"csv\"), schema: Some(\"\"), options: {\"header\": \"True\", \"sep\": \",\", \"inferSchema\": \"True\"}, paths: [\"/Users/andrewgazelka/.cache/kagglehub/datasets/chitwanmanchanda/fraudulent-transactions-data/versions/1\"], predicates: [] })) })) }), rename_columns_map: {\"oldbalanceOrg\": \"oldBalanceOrig\"}, renames: [] })) }), rename_columns_map: {\"newbalanceOrig\": \"newBalanceOrig\"}, renames: [] })) }), rename_columns_map: {\"oldbalanceDest\": \"oldBalanceDest\"}, renames: [] })) }), rename_columns_map: {\"newbalanceDest\": \"newBalanceDest\"}, renames: [] })) }), rename_columns_map: {\"oldbalanceOrg\": \"oldBalanceOrig\"}, renames: [] })) }), rename_columns_map: {\"newbalanceOrig\": \"newBalanceOrig\"}, renames: [] })) }), rename_columns_map: {\"oldbalanceDest\": \"oldBalanceDest\"}, renames: [] })) }), rename_columns_map: {\"newbalanceDest\": \"newBalanceDest\"}, renames: [] })\n\nLocation:\n    src/daft-connect/src/translation/logical_plan.rs:132:21\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:15003 {grpc_message:\"Error in Daft server: Failed to show string\\n\\nCaused by:\\n    Unsupported relation type: WithColumnsRenamed(WithColumnsRenamed { input: Some(Relation { common: Some(RelationCommon { source_info: \\\"\\\", plan_id: Some(14), origin: None }), rel_type: Some(WithColumnsRenamed(WithColumnsRenamed { input: Some(Relation { common: Some(RelationCommon { source_info: \\\"\\\", plan_id: Some(13), origin: None }), rel_type: Some(WithColumnsRenamed(WithColumnsRenamed { input: Some(Relation { common: Some(RelationCommon { source_info: \\\"\\\", plan_id: Some(12), origin: None }), rel_type: Some(WithColumnsRenamed(WithColumnsRenamed { input: Some(Relation { common: Some(RelationCommon { source_info: \\\"\\\", plan_id: Some(11), origin: None }), rel_type: Some(WithColumnsRenamed(WithColumnsRenamed { input: Some(Relation { common: Some(RelationCommon { source_info: \\\"\\\", plan_id: Some(10), origin: None }), rel_type: Some(WithColumnsRenamed(WithColumnsRenamed { input: Some(Relation { common: Some(RelationCommon { source_info: \\\"\\\", plan_id: Some(9), origin: None }), rel_type: Some(WithColumnsRenamed(WithColumnsRenamed { input: Some(Relation { common: Some(RelationCommon { source_info: \\\"\\\", plan_id: Some(8), origin: None }), rel_type: Some(WithColumnsRenamed(WithColumnsRenamed { input: Some(Relation { common: Some(RelationCommon { source_info: \\\"\\\", plan_id: Some(6), origin: None }), rel_type: Some(Read(Read { is_streaming: false, read_type: Some(DataSource(DataSource { format: Some(\\\"csv\\\"), schema: Some(\\\"\\\"), options: {\\\"header\\\": \\\"True\\\", \\\"sep\\\": \\\",\\\", \\\"inferSchema\\\": \\\"True\\\"}, paths: [\\\"/Users/andrewgazelka/.cache/kagglehub/datasets/chitwanmanchanda/fraudulent-transactions-data/versions/1\\\"], predicates: [] })) })) }), rename_columns_map: {\\\"oldbalanceOrg\\\": \\\"oldBalanceOrig\\\"}, renames: [] })) }), rename_columns_map: {\\\"newbalanceOrig\\\": \\\"newBalanceOrig\\\"}, renames: [] })) }), rename_columns_map: {\\\"oldbalanceDest\\\": \\\"oldBalanceDest\\\"}, renames: [] })) }), rename_columns_map: {\\\"newbalanceDest\\\": \\\"newBalanceDest\\\"}, renames: [] })) }), rename_columns_map: {\\\"oldbalanceOrg\\\": \\\"oldBalanceOrig\\\"}, renames: [] })) }), rename_columns_map: {\\\"newbalanceOrig\\\": \\\"newBalanceOrig\\\"}, renames: [] })) }), rename_columns_map: {\\\"oldbalanceDest\\\": \\\"oldBalanceDest\\\"}, renames: [] })) }), rename_columns_map: {\\\"newbalanceDest\\\": \\\"newBalanceDest\\\"}, renames: [] })\\n\\nLocation:\\n    src/daft-connect/src/translation/logical_plan.rs:132:21\", grpc_status:13, created_time:\"2024-12-19T08:54:10.208962-08:00\"}\"\n>",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mSparkConnectGrpcException\u001b[0m                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 7\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m old_col, new_col \u001b[38;5;129;01min\u001b[39;00m corrected_cols\u001b[38;5;241m.\u001b[39mitems():\n\u001b[1;32m      5\u001b[0m     df \u001b[38;5;241m=\u001b[39m df\u001b[38;5;241m.\u001b[39mwithColumnRenamed(old_col, new_col)\n\u001b[0;32m----> 7\u001b[0m \u001b[43mdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshow\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Projects/daft-connect-example/.venv/lib/python3.12/site-packages/pyspark/sql/connect/dataframe.py:996\u001b[0m, in \u001b[0;36mDataFrame.show\u001b[0;34m(self, n, truncate, vertical)\u001b[0m\n\u001b[1;32m    995\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mshow\u001b[39m(\u001b[38;5;28mself\u001b[39m, n: \u001b[38;5;28mint\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m20\u001b[39m, truncate: Union[\u001b[38;5;28mbool\u001b[39m, \u001b[38;5;28mint\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m, vertical: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 996\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_show_string\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtruncate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvertical\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[0;32m~/Projects/daft-connect-example/.venv/lib/python3.12/site-packages/pyspark/sql/connect/dataframe.py:753\u001b[0m, in \u001b[0;36mDataFrame._show_string\u001b[0;34m(self, n, truncate, vertical)\u001b[0m\n\u001b[1;32m    741\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m:\n\u001b[1;32m    742\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m PySparkTypeError(\n\u001b[1;32m    743\u001b[0m             error_class\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNOT_BOOL\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    744\u001b[0m             message_parameters\u001b[38;5;241m=\u001b[39m{\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    747\u001b[0m             },\n\u001b[1;32m    748\u001b[0m         )\n\u001b[1;32m    750\u001b[0m pdf \u001b[38;5;241m=\u001b[39m \u001b[43mDataFrame\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwithPlan\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    751\u001b[0m \u001b[43m    \u001b[49m\u001b[43mplan\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mShowString\u001b[49m\u001b[43m(\u001b[49m\u001b[43mchild\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_plan\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_rows\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtruncate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_truncate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvertical\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvertical\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    752\u001b[0m \u001b[43m    \u001b[49m\u001b[43msession\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_session\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m--> 753\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtoPandas\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    754\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m pdf \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    755\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m pdf[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mshow_string\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[0;32m~/Projects/daft-connect-example/.venv/lib/python3.12/site-packages/pyspark/sql/connect/dataframe.py:1663\u001b[0m, in \u001b[0;36mDataFrame.toPandas\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1661\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot collect on empty session.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   1662\u001b[0m query \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_plan\u001b[38;5;241m.\u001b[39mto_proto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_session\u001b[38;5;241m.\u001b[39mclient)\n\u001b[0;32m-> 1663\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_session\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclient\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_pandas\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Projects/daft-connect-example/.venv/lib/python3.12/site-packages/pyspark/sql/connect/client/core.py:873\u001b[0m, in \u001b[0;36mSparkConnectClient.to_pandas\u001b[0;34m(self, plan)\u001b[0m\n\u001b[1;32m    869\u001b[0m (self_destruct_conf,) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_config_with_defaults(\n\u001b[1;32m    870\u001b[0m     (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mspark.sql.execution.arrow.pyspark.selfDestruct.enabled\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfalse\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m    871\u001b[0m )\n\u001b[1;32m    872\u001b[0m self_destruct \u001b[38;5;241m=\u001b[39m cast(\u001b[38;5;28mstr\u001b[39m, self_destruct_conf)\u001b[38;5;241m.\u001b[39mlower() \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrue\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m--> 873\u001b[0m table, schema, metrics, observed_metrics, _ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execute_and_fetch\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    874\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mself_destruct\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mself_destruct\u001b[49m\n\u001b[1;32m    875\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    876\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m table \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    878\u001b[0m schema \u001b[38;5;241m=\u001b[39m schema \u001b[38;5;129;01mor\u001b[39;00m from_arrow_schema(table\u001b[38;5;241m.\u001b[39mschema, prefer_timestamp_ntz\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m~/Projects/daft-connect-example/.venv/lib/python3.12/site-packages/pyspark/sql/connect/client/core.py:1283\u001b[0m, in \u001b[0;36mSparkConnectClient._execute_and_fetch\u001b[0;34m(self, req, self_destruct)\u001b[0m\n\u001b[1;32m   1280\u001b[0m schema: Optional[StructType] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1281\u001b[0m properties: Dict[\u001b[38;5;28mstr\u001b[39m, Any] \u001b[38;5;241m=\u001b[39m {}\n\u001b[0;32m-> 1283\u001b[0m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mresponse\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execute_and_fetch_as_iterator\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreq\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m   1284\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43misinstance\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mresponse\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mStructType\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m   1285\u001b[0m \u001b[43m        \u001b[49m\u001b[43mschema\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mresponse\u001b[49m\n",
      "File \u001b[0;32m~/Projects/daft-connect-example/.venv/lib/python3.12/site-packages/pyspark/sql/connect/client/core.py:1264\u001b[0m, in \u001b[0;36mSparkConnectClient._execute_and_fetch_as_iterator\u001b[0;34m(self, req)\u001b[0m\n\u001b[1;32m   1262\u001b[0m                     \u001b[38;5;28;01myield from\u001b[39;00m handle_response(b)\n\u001b[1;32m   1263\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m error:\n\u001b[0;32m-> 1264\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_handle_error\u001b[49m\u001b[43m(\u001b[49m\u001b[43merror\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Projects/daft-connect-example/.venv/lib/python3.12/site-packages/pyspark/sql/connect/client/core.py:1503\u001b[0m, in \u001b[0;36mSparkConnectClient._handle_error\u001b[0;34m(self, error)\u001b[0m\n\u001b[1;32m   1490\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1491\u001b[0m \u001b[38;5;124;03mHandle errors that occur during RPC calls.\u001b[39;00m\n\u001b[1;32m   1492\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1500\u001b[0m \u001b[38;5;124;03mThrows the appropriate internal Python exception.\u001b[39;00m\n\u001b[1;32m   1501\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(error, grpc\u001b[38;5;241m.\u001b[39mRpcError):\n\u001b[0;32m-> 1503\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_handle_rpc_error\u001b[49m\u001b[43m(\u001b[49m\u001b[43merror\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1504\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(error, \u001b[38;5;167;01mValueError\u001b[39;00m):\n\u001b[1;32m   1505\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot invoke RPC\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(error) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mclosed\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(error):\n",
      "File \u001b[0;32m~/Projects/daft-connect-example/.venv/lib/python3.12/site-packages/pyspark/sql/connect/client/core.py:1543\u001b[0m, in \u001b[0;36mSparkConnectClient._handle_rpc_error\u001b[0;34m(self, rpc_error)\u001b[0m\n\u001b[1;32m   1541\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m SparkConnectGrpcException(status\u001b[38;5;241m.\u001b[39mmessage) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1542\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1543\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m SparkConnectGrpcException(\u001b[38;5;28mstr\u001b[39m(rpc_error)) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[0;31mSparkConnectGrpcException\u001b[0m: <_MultiThreadedRendezvous of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"Error in Daft server: Failed to show string\n\nCaused by:\n    Unsupported relation type: WithColumnsRenamed(WithColumnsRenamed { input: Some(Relation { common: Some(RelationCommon { source_info: \"\", plan_id: Some(14), origin: None }), rel_type: Some(WithColumnsRenamed(WithColumnsRenamed { input: Some(Relation { common: Some(RelationCommon { source_info: \"\", plan_id: Some(13), origin: None }), rel_type: Some(WithColumnsRenamed(WithColumnsRenamed { input: Some(Relation { common: Some(RelationCommon { source_info: \"\", plan_id: Some(12), origin: None }), rel_type: Some(WithColumnsRenamed(WithColumnsRenamed { input: Some(Relation { common: Some(RelationCommon { source_info: \"\", plan_id: Some(11), origin: None }), rel_type: Some(WithColumnsRenamed(WithColumnsRenamed { input: Some(Relation { common: Some(RelationCommon { source_info: \"\", plan_id: Some(10), origin: None }), rel_type: Some(WithColumnsRenamed(WithColumnsRenamed { input: Some(Relation { common: Some(RelationCommon { source_info: \"\", plan_id: Some(9), origin: None }), rel_type: Some(WithColumnsRenamed(WithColumnsRenamed { input: Some(Relation { common: Some(RelationCommon { source_info: \"\", plan_id: Some(8), origin: None }), rel_type: Some(WithColumnsRenamed(WithColumnsRenamed { input: Some(Relation { common: Some(RelationCommon { source_info: \"\", plan_id: Some(6), origin: None }), rel_type: Some(Read(Read { is_streaming: false, read_type: Some(DataSource(DataSource { format: Some(\"csv\"), schema: Some(\"\"), options: {\"header\": \"True\", \"sep\": \",\", \"inferSchema\": \"True\"}, paths: [\"/Users/andrewgazelka/.cache/kagglehub/datasets/chitwanmanchanda/fraudulent-transactions-data/versions/1\"], predicates: [] })) })) }), rename_columns_map: {\"oldbalanceOrg\": \"oldBalanceOrig\"}, renames: [] })) }), rename_columns_map: {\"newbalanceOrig\": \"newBalanceOrig\"}, renames: [] })) }), rename_columns_map: {\"oldbalanceDest\": \"oldBalanceDest\"}, renames: [] })) }), rename_columns_map: {\"newbalanceDest\": \"newBalanceDest\"}, renames: [] })) }), rename_columns_map: {\"oldbalanceOrg\": \"oldBalanceOrig\"}, renames: [] })) }), rename_columns_map: {\"newbalanceOrig\": \"newBalanceOrig\"}, renames: [] })) }), rename_columns_map: {\"oldbalanceDest\": \"oldBalanceDest\"}, renames: [] })) }), rename_columns_map: {\"newbalanceDest\": \"newBalanceDest\"}, renames: [] })\n\nLocation:\n    src/daft-connect/src/translation/logical_plan.rs:132:21\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:15003 {grpc_message:\"Error in Daft server: Failed to show string\\n\\nCaused by:\\n    Unsupported relation type: WithColumnsRenamed(WithColumnsRenamed { input: Some(Relation { common: Some(RelationCommon { source_info: \\\"\\\", plan_id: Some(14), origin: None }), rel_type: Some(WithColumnsRenamed(WithColumnsRenamed { input: Some(Relation { common: Some(RelationCommon { source_info: \\\"\\\", plan_id: Some(13), origin: None }), rel_type: Some(WithColumnsRenamed(WithColumnsRenamed { input: Some(Relation { common: Some(RelationCommon { source_info: \\\"\\\", plan_id: Some(12), origin: None }), rel_type: Some(WithColumnsRenamed(WithColumnsRenamed { input: Some(Relation { common: Some(RelationCommon { source_info: \\\"\\\", plan_id: Some(11), origin: None }), rel_type: Some(WithColumnsRenamed(WithColumnsRenamed { input: Some(Relation { common: Some(RelationCommon { source_info: \\\"\\\", plan_id: Some(10), origin: None }), rel_type: Some(WithColumnsRenamed(WithColumnsRenamed { input: Some(Relation { common: Some(RelationCommon { source_info: \\\"\\\", plan_id: Some(9), origin: None }), rel_type: Some(WithColumnsRenamed(WithColumnsRenamed { input: Some(Relation { common: Some(RelationCommon { source_info: \\\"\\\", plan_id: Some(8), origin: None }), rel_type: Some(WithColumnsRenamed(WithColumnsRenamed { input: Some(Relation { common: Some(RelationCommon { source_info: \\\"\\\", plan_id: Some(6), origin: None }), rel_type: Some(Read(Read { is_streaming: false, read_type: Some(DataSource(DataSource { format: Some(\\\"csv\\\"), schema: Some(\\\"\\\"), options: {\\\"header\\\": \\\"True\\\", \\\"sep\\\": \\\",\\\", \\\"inferSchema\\\": \\\"True\\\"}, paths: [\\\"/Users/andrewgazelka/.cache/kagglehub/datasets/chitwanmanchanda/fraudulent-transactions-data/versions/1\\\"], predicates: [] })) })) }), rename_columns_map: {\\\"oldbalanceOrg\\\": \\\"oldBalanceOrig\\\"}, renames: [] })) }), rename_columns_map: {\\\"newbalanceOrig\\\": \\\"newBalanceOrig\\\"}, renames: [] })) }), rename_columns_map: {\\\"oldbalanceDest\\\": \\\"oldBalanceDest\\\"}, renames: [] })) }), rename_columns_map: {\\\"newbalanceDest\\\": \\\"newBalanceDest\\\"}, renames: [] })) }), rename_columns_map: {\\\"oldbalanceOrg\\\": \\\"oldBalanceOrig\\\"}, renames: [] })) }), rename_columns_map: {\\\"newbalanceOrig\\\": \\\"newBalanceOrig\\\"}, renames: [] })) }), rename_columns_map: {\\\"oldbalanceDest\\\": \\\"oldBalanceDest\\\"}, renames: [] })) }), rename_columns_map: {\\\"newbalanceDest\\\": \\\"newBalanceDest\\\"}, renames: [] })\\n\\nLocation:\\n    src/daft-connect/src/translation/logical_plan.rs:132:21\", grpc_status:13, created_time:\"2024-12-19T08:54:10.208962-08:00\"}\"\n>"
     ]
    }
   ],
   "source": [
    "# Rename columns\n",
    "corrected_cols = {\n",
    "    \"oldbalanceOrg\": \"oldBalanceOrig\",\n",
    "    \"newbalanceOrig\": \"newBalanceOrig\",\n",
    "    \"oldbalanceDest\": \"oldBalanceDest\",\n",
    "    \"newbalanceDest\": \"newBalanceDest\",\n",
    "}\n",
    "for old_col, new_col in corrected_cols.items():\n",
    "    df = df.withColumnRenamed(old_col, new_col)\n",
    "\n",
    "df.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
